{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 대용량 데이터 처리 기술 개요\n",
    "## 1. 빅데이터?\n",
    "- 빅데이터란 기존 데이터베이스 관리도구의 능력을 넘어서는 대량의 정형 또는 심지어 데이터베이스 형태가 아닌 비정형의 데이터 집합조차 데이터로부터 가치를 추출하고 분석하는 기술이다. - 위키피디아\n",
    "\n",
    "### 1.1. 기존 데이터베이스 관리도구\n",
    "    - SQL 기반의 데이터베이스\n",
    "    - 주로 컴퓨터 1대에서 돌아감, 고성능이 필요하면 좋은(비싼) 컴퓨터 사용\n",
    "    - 컴퓨터 1대로 처리할 수 있는 용량과 성능의 한계\n",
    "\n",
    "### 1.2. 대량\n",
    "    - 컴퓨터 1대로 처리할 수 없는 양 (수십TB 이상)\n",
    "    - 3V (by IBM) : Volume, Velocity, Variety\n",
    "    \n",
    "### 1.3. 기술\n",
    "    - 컴퓨터 1대로 처리하지 못하므로, 여러 대를 연결해서 데이터를 저장하고 처리하는 심플한 접근\n",
    "    - 주로 구글 등 검색엔진 회사들이 웹 전체를 저장하고 처리하려다보니 기술개발이 필요하게됨\n",
    "    - 구글이 이끌고, 야후 등이 오픈 소스를 통해 (하둡) 적극 지원, 접근하기 쉬워지고 널리 쓰이기 시작\n",
    "    - 빅데이터 기술 = 대부분 하둡이라고 생각해도 무방\n",
    "    \n",
    "### 1.4. 형태 (정형, 비정형)\n",
    "    - SQL 기반의 데이터는 거의 행렬 형태로 정형화된 데이터였으나 일반 문서 (웹 문서) 등과 같이 비정형화된 데이터도 초점\n",
    "    \n",
    "### 1.5. 가치 추출, 결과 분석\n",
    "    - 데이터를 저장만 해서는 쓸모가 없음\n",
    "    - 데이터를 읽어들이고, 변환하고, 핵심을 추출하는 것도 마찬가지로 컴퓨터 1대로 할 수 있는 것보다 훨씬 빨라져야 함\n",
    "    - 맵리듀스 (MapReduce) : 분산 데이터 처리\n",
    "    - 현재는 스파크 (Apache Spark)가 널리 쓰임\n",
    "    \n",
    "## 2. 빅데이터 기술\n",
    "- 데이터가 커졌을 때, 이를 처리하는 방식\n",
    "    - 고전적으로는 큐잉, 샤딩(데이터의 키를 해시하여 여러 대의 데이터베이스로 분산시킴) 등의 여러가지 테크닉으로 큰 양의 데이터를 처리\n",
    "        - 큐(que) : 처리해야할 task를 처리하기 위해 줄 세우기\n",
    "        - 샤드(shard) : 예를 들어, Facebook에서 회원수가 많아서, 회원을 특정 기준에 의해 나누어 저장한다.\n",
    "        - 해시(hash) : 뭉갠다. user id를 다른 숫자로 나오게 처리하는 방식\n",
    "        - ex) DB 1대로 쓰다가 꽉차서, 10대로 늘리자 하면, user id 끝자리 0~9번을 각 서버에 저장... 아이디를 다른 방식으로 변환한 다음에 변환한 기준을 가지고 ...?\n",
    "    - 이러한 방법들은 갈수록 시스템의 복잡도를 증가시키고, 문제가 생겼을 때 유지보수하기 매우 힘듬\n",
    "    - 이를 대체하기 위해 **스스로 데이터를 분산**시키고, **오류로부터 데이터를 복구하는 기능을 가진 시스템**들이 만들어짐\n",
    "    \n",
    "## 3. 빅데이터의 시초 : GFS\n",
    "- Google File System 논문 (2003년)\n",
    "    - File System : 디렉토리 구조가 있고, 파일을 저장하거나, 옮기거나, 이름을 변경하거나, 저장할 수 있는 시스템.\n",
    "- 막대한 양의 웹 문서를 저장 조회해야하는데, 컴퓨터 1대로는 당연히 처리가 불가능\n",
    "- 저렴한 하드웨어를 사용하면서, 대신 중복저장을 통해 파일 유실을 방지\n",
    "    - [S3의 파일이 유실되지 않을 가능성 nine-eleven(99.999999999%)](https://www.quora.com/Has-Amazon-S3-ever-lost-data-permanently)\n",
    "    <img src = \"../../images/dees_ksw_3_1.png\">\n",
    "    \n",
    "- 파일을 새로 추가하는데 집중, 삭제나 파일 덮어쓰기는 어려움\n",
    "- Latency 보다 Throughput을 중시\n",
    "    - 둘 다 속도의 개념\n",
    "    - Latency : 요청했을 때, 얼마나 빨리 받아주느냐\n",
    "    - Throughput : 얼마의 용량을 처리할 수 있느냐\n",
    "    - 수도꼭지의 비유\n",
    "        - Latency : 물이 얼마만에 나오느냐 ... 개인용 시스템에 중요함\n",
    "        - Throughput : 물이 얼마나 콸콸콸 나오느냐, 찔끔 찔끔 나오면 Throughput이 낮은 것.\n",
    "    - Throughput을 중시하는 이유 : PageRank 때문에 ...\n",
    "        <img src = \"../../images/dees_ksw_3_2.jpg\">\n",
    "    \n",
    "        - [PageRank](https://en.wikipedia.org/wiki/PageRank)\n",
    "            - DB에 WebPage를 저장\n",
    "            - 특정 사이트가 여러 페이지에서 링크가 많이 됐으면, 그만큼의 중요성이 높다고 판단.\n",
    "            \n",
    "- 클러스터 댓수를 늘릴수록 저장용량과 throughput이 점점 올라감\n",
    "\n",
    "<img src = \"../../images/dees_ksw_3_3.png\">\n",
    "\n",
    "- 여러 대의 Chunk Server에 중복 저장\n",
    "- Master를 통해 파일의 위치를 알아내고, Chunk Server에 직접 접속해서 데이터 전송받음\n",
    "\n",
    "\n",
    "- Master : Master\n",
    "    - 무슨 파일이 어디 서버에 있는지 알고 있고, APP에 알려준다.\n",
    "    - 마스터에 볼일이 끝나서 App이 Chunk Server에 요청\n",
    "- Chunk Server : Slave\n",
    "    - Chunk Server가 고장났을 떄, data를 안정적으로 유지하기 위해서 Redundant.\n",
    "    \n",
    "\n",
    "## 4. 빅데이터의 시초 : MapReduce\n",
    "\n",
    "## 5. 빅데이터의 시초 : Hadoop\n",
    "\n",
    "### 5.1. Hadoop HDFS\n",
    "\n",
    "### 5.2. HDFS Shell API\n",
    "\n",
    "## 6. MapReduce\n",
    "### 6.1. MapReduce 개념\n",
    "### 6.2. MapReduce 워크플로우\n",
    "### 6.3. MapReduce 코드\n",
    "\n",
    "## 7. Hadoop Hive\n",
    "## 8. 분산 데이터베이스 (NoSQL)\n",
    "## 9. Apache HBase\n",
    "## 10. HBase 아키텍쳐\n",
    "\n",
    "## 11. 하둡 생태계의 많은 프로젝트들\n",
    "### 11.1 Apache Spark\n",
    "### 11.1. Apache Storm\n",
    "### 11.2. Spark Streaming\n",
    "\n",
    "## 12. Spark 이후의 기술들?\n",
    "### 12.1. Apache Flink\n",
    "### 12.2. 데이터 수집기\n",
    "### 12.3. Apache Flume\n",
    "### 12.4. Apache Kafka\n",
    "### 12.5. Apache AirFlow\n",
    "\n",
    "## 13. 그 밖의 빅데이터 프로젝트들\n",
    "### 13.1. 검색 엔진을 넘어선 ElasticSearch\n",
    "\n",
    "## 빅데이터 분석 워크플로우\n",
    "## 향후 방향성\n",
    "## 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
